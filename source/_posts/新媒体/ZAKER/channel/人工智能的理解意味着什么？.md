
---
title: '人工智能的理解意味着什么？'
categories: 
 - 新媒体
 - ZAKER
 - channel
headimg: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61c693268e9f0901e855734c_1024.jpg'
author: ZAKER
comments: false
date: Fri, 24 Dec 2021 20:30:00 GMT
thumbnail: 'https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61c693268e9f0901e855734c_1024.jpg'
---

<div>   
<p>编辑 | 萝卜皮</p><p></p><div class="img_box" id="id_imagebox_0" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_0" data-original="http://zkres1.myzaker.com/202112/61c693268e9f0901e855734c_1024.jpg" data-height="575" data-width="1080" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61c693268e9f0901e855734c_1024.jpg" referrerpolicy="no-referrer"></div></div>记住 IBM 的 Watson，AI Jeopardy！冠军？2010 年的一项促销活动宣称：「Watson 理解自然语言的所有歧义和复杂性。」然而，正如我们在 Watson 随后在寻求「用人工智能彻底改变医学」的过程中惨败时所看到的那样，语言设施的表面与实际理解人类语言并不相同。<p></p><p>自然语言理解长期以来一直是人工智能研究的主要目标。起初，研究人员试图手动编程机器理解新闻故事、小说或人类可能写的任何其他内容所需的一切。正如沃森所展示的那样，这种方法是徒劳的——不可能写下理解文本所需的所有不成文的事实、规则和假设。最近，一个新的范式已经建立：我们不是建立显性知识，而是让机器学习自己理解语言，只需摄取大量书面文本并学习预测单词即可。结果就是研究人员所说的语言模型。当基于大型神经网络（如 OpenAI 的 GPT-3）时，此类模型可以生成令人难以置信的人类散文和诗歌，并且似乎执行复杂的语言推理。</p><p>但是 GPT-3 ——对来自数千个网站、书籍和百科全书的文本进行训练——是否超越了 Watson 的表面？它真的理解它生成的语言和表面上的推理吗？这是 AI 研究界存在明显分歧的话题。此类讨论曾经是哲学家的职权范围，但在过去十年中，人工智能已经从学术泡沫中冲破到现实世界，它对现实世界的缺乏了解可能会产生真实的、有时甚至是毁灭性的后果。在一项研究中，IBM 的 Watson 被发现提出了「不安全和不正确的治疗建议的多个例子」。另一项研究表明，谷歌的机器翻译系统在为非英语患者翻译医疗说明时出现了重大错误。</p><p>我们如何在实践中确定机器是否可以理解？1950 年，计算先驱艾伦图灵试图用他著名的「模仿游戏」来回答这个问题，现在称为图灵测试。一台机器和一个人都隐藏在视线之外，将竞争说服人类判断他们的人性，仅使用对话。如果法官无法分辨哪个是人类，那么，图灵断言，我们应该认为机器正在思考——实际上，是理解。</p><p>不幸的是，图灵低估了人类被机器愚弄的倾向。即使是简单的聊天机器人，比如 Joseph Weizenbaum 1960 年代的心理治疗师 Eliza，也会让人们相信他们正在与一个理解性的人交谈，即使他们知道他们的对话伙伴是一台机器。</p><p>在 2012 年的一篇论文中，计算机科学家 Hector Levesque、Ernest Davis 和 Leora Morgenstern 提出了一个更客观的测试，他们称之为 Winograd 模式挑战。</p><p></p><div class="img_box" id="id_imagebox_1" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_1" data-original="http://zkres1.myzaker.com/202112/61c693268e9f0901e855734d_1024.jpg" data-height="209" data-width="676" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61c693268e9f0901e855734d_1024.jpg" referrerpolicy="no-referrer"></div></div>论文链接：https://nyuscholars.nyu.edu/en/publications/the-winograd-schema-challenge-2<p></p><p>此测试已被 AI 语言社区采用，作为评估机器理解的一种方式，也许是最好的方式——尽管我们将看到，它并不完美。以语言研究员 Terry Winograd 的名字命名的 Winograd 模式由一对句子组成，这些句子仅相差一个词，每个句子后跟一个问题。这里有两个例子：</p><p>句子 1：我把瓶子里的水倒进杯子里，直到它满了。</p><p>问题：什么是满的，瓶子还是杯子？</p><p>句子 2：我把瓶子里的水倒进杯子里，直到它空了。</p><p>问题：什么是空的，瓶子还是杯子？</p><p>第 1 句：乔的叔叔在网球上仍然可以击败他，尽管他已经 30 岁了。</p><p>问题：谁大，乔还是乔的叔叔？</p><p>第 2 句：乔的叔叔在网球上仍然可以击败他，即使他比他年轻 30 岁。</p><p>问题：谁更年轻，乔还是乔的叔叔？</p><p>在每个句子对中，一个词的差异可以改变代词所指的事物或人。正确回答这些问题似乎需要常识性的理解。Winograd 模式正是为了测试这种理解而设计的，减轻了图灵测试对不可靠的人类判断或聊天机器人技巧的脆弱性。特别是，作者设计了数百个他们认为是「谷歌证明」的模式：机器不应该能够使用谷歌搜索（或类似搜索）来正确回答问题。</p><p>这些模式是 2016 年举办的一场比赛的主题，其中获胜程序仅在 58% 的句子上是正确的——这几乎没有比猜到的更好的结果。领先的 AI 研究员 Oren Etzioni 打趣道：「当 AI 无法确定『它』在句子中指的是什么时，很难相信它会接管世界。」</p><p>然而，由于大型神经网络语言模型的出现，人工智能程序解决 Winograd 模式的能力迅速提高。OpenAI 2020 年的一篇论文报告称，GPT-3 在一组基准 Winograd 模式中的近 90% 的句子上都是正确的。</p><p></p><div class="img_box" id="id_imagebox_2" onclick><div class="content_img_div perview_img_div"><img class="lazy opacity_0 " id="img_2" data-original="http://zkres1.myzaker.com/202112/61c693268e9f0901e855734e_1024.jpg" data-height="269" data-width="601" src="https://cors.zfour.workers.dev/?http://zkres1.myzaker.com/202112/61c693268e9f0901e855734e_1024.jpg" referrerpolicy="no-referrer"></div></div>论文链接：https://arxiv.org/abs/2005.14165<p></p><p>在专门针对这些任务进行训练后，其他语言模型的表现甚至更好。近期，神经网络语言模型在一组特定的 Winograd 模式上达到了约 97% 的准确率，这些模式是称为 SuperGLUE 的 AI 语言理解竞赛的一部分。这种准确性大致相当于人类的表现。这是否意味着神经网络语言模型已经达到了人类的理解水平？</p><p>不必要。尽管创作者尽了最大努力，但那些 Winograd 模式实际上并没有经过 Google 验证。这些挑战，就像许多其他当前的 AI 语言理解测试一样，有时允许走捷径，让神经网络在不理解的情况下也能表现良好。例如，考虑「跑车通过邮件卡车，因为它快了」和「跑车通过邮件卡车，因为它是慢」。一个在大量英语句子语料库上训练的语言模型将吸收「跑车」和「快」以及「邮车」和「慢」之间的相关性，因此它可以仅根据这些相关性而不是通过任何理解来正确回答。事实证明，SuperGLUE 比赛中的许多 Winograd 模式都允许这些类型的统计相关性。</p><p>艾伦人工智能研究所的一组研究人员并没有放弃 Winograd 模式作为理解测试，而是决定尝试解决他们的一些问题。2019 年，他们创建了 WinoGrande，这是一组更大的 Winograd 模式。WinoGrande 包含多达 44,000 个句子，而不是数百个示例。为了获得如此多的例子，研究人员求助于 Amazon Mechanical Turk，这是一个流行的众包工作平台。每个（人类）工人被要求写几个句子对，有一些限制以确保集合包含不同的主题，尽管现在每对中的句子可能相差不止一个单词。</p><p>然后，研究人员试图通过将相对简单的 AI 方法应用于每个句子，并丢弃任何太容易解决的句子来消除可能允许统计捷径的句子。正如预期的那样，与原始的 Winograd 模式集合相比，剩余的句子对机器提出了更困难的挑战。虽然人类的得分仍然很高，但在原始集上与人类表现相匹配的神经网络语言模型在 WinoGrande 集上的得分要低得多。这个新挑战似乎将 Winograd 模式作为常识理解的测试——只要仔细筛选句子以确保它们是谷歌证明的。</p><p>然而，另一个惊喜还在后面。在 WinoGrande 系列出版后的近两年里，神经网络语言模型变得越来越大，而且它们越大，它们似乎在这个新挑战中的得分就越高。在撰写本文时，当前最好的程序——已经在 TB 级文本上训练，然后在数千个 WinoGrande 示例上进一步训练——接近 90% 的正确率（人类获得大约 94% 的正确率）。这种性能的提高，几乎完全是由于神经网络语言模型及其训练数据的大小增加。</p><p>这些越来越大的网络最终是否达到了人类的常识性理解？同样，这不太可能。WinoGrande 的结果带有一些重要的警告。例如，由于句子依赖于 Amazon Mechanical Turk 工人，因此写作的质量和连贯性非常参差不齐。此外，用于剔除「非谷歌证明」句子的「简单」人工智能方法可能过于简单，无法找出大型神经网络可用的所有可能的统计捷径，并且只适用于单个句子，因此剩余的一些句子最终失去了「孪生子」。一项后续研究表明，仅在双句子上测试的神经网络语言模型——并且要求两者都正确——比人类准确得多，这表明早期 90% 的结果并不像看起来那么重要。</p><p>那么，如何理解 Winograd 传奇呢？主要的教训是，通常很难从他们在给定挑战中的表现确定 AI 系统是否真正理解他们处理的语言（或其他数据）。我们现在知道，神经网络经常使用统计捷径——而不是实际展示人类的理解能力——在 Winograd 模式以及许多最流行的「通用语言理解」基准上获得高性能。</p><p>在 ScienceAI 看来，问题的关键在于理解语言需要理解世界，而只接触语言的机器无法获得这样的理解。考虑一下理解「跑车通过邮车因为它开得更慢」的含义。你需要知道什么是跑车和邮车，汽车可以相互「通过」，并且在更基本的层面上，车辆是存在于世界上并与之互动的物体，由人类按照自己的议程驱动。</p><p>所有这些都是我们人类认为理所当然的知识，但它并没有内置到机器中，也没有可能明确地写在任何语言模型的训练文本中。一些认知科学家认为，人类依赖于空间、时间和世界的许多其他基本属性的先天的、前语言的核心知识来学习和理解语言。如果我们想让机器同样掌握人类语言，我们首先需要赋予它们人类与生俱来的原始原则。为了评估机器的理解，我们应该从评估他们对这些基本规则的掌握程度开始，人们可以称之为「婴儿形而上学」。</p><p>与 Watson 和 GPT-3 等人工智能系统的惊人壮举相比，训练和评估婴儿级智能的机器似乎是一个巨大的倒退。但是，如果真正和可信的理解是目标，这可能是机器能够真正理解句子中「它」所指的内容以及理解「它」所包含的一切的唯一途径。</p><p><strong></strong></p><div id="recommend_bottom"></div><div id="article_bottom"></div>  
</div>
            